# ML-Task
The objective of this machine learning task is to build a model which predicts if a website is a phishing website. This is a classification task which is a supervised machine learning approach. The dataset is provided with features and labels of target variable. As in any other classification task, the target variable here - Result, is categorical with two possible values 0 and 1. Hence, this is a binary classification task. In fact, all the features in the dataset are also categorical variables. 

Basic data exploration provides some fundamental information such as structure of the data, shape of the dataframe, data types of columns and count of missing values.It also suggests that the data is clean, that is, there are no missing values and all the categorical variables are already encoded. The value count of the target variable shows that the classes are balanced, that is, there is no class imbalance. This is helpful in selecting preliminary evaluation methods. For example, suppose classes are imbalanced and 99% of the websites are not phishing then a model which classifies every website as phishing has a 99% accuracy score but it will not serve our purpose. Hence, accuracy score is not a suitable metric. However, this is not the case here so accuracy score can be used as an evaluation metric for preliminary model evaluation. 

This is followed by some data pre-processing in which the features and labels are created from the dataset and the data is divided into training data and test data. The split is validated by checking the shape of the features array and labels array. The test data is unused until the final model is selected and the final model is validated on this test data which is totally unseen. For model evaluation to select the best model a validation dataset is created from the training data itself using the cross validation technique. This is done to prevent data leakage and get a fair validation of the model by evaluating it on totally unseen data. 

A set of classification models are developed - Logistic Regression model, Support Vector Machine Model, Decision Tree Model and KNN model. The accuracy of each model is evaluated on the validation set using cross validation. A boxplot is generated to visualize the results. It is observed that the Support Vector machine model performs the best with a median accuracy of 95.81%, the median is chosen as it is less affected by the outliers. The Decision Tree classifier is close second.

An ensemble learning model is developed which is basically a combination of all the individual classification models to check if it gives better performance as the ensemble learning classifier overcomes the variance of the individual models and uses majority voting to arrive at a prediction. However, there is no significant improvement in model performance. Bagging is used to develop a bagging classifier with SVM as the base estimator as it was the most accurate individual mdoel. Bagging is the machine learning techniques in which multiple machine learning models are trained on different subsets of the training data to overcome overfitting, improve the model performance by reducing variance and combine predictions from multiple models. However, in this case too there is no  significant improvement in the model performance.


A random Forest model is developed and evaluated. As the indiviual decision tree model generated failry accurate predictions, it is imperative to check the model performance of a Random Forest model. The Random Forest is an extension of bagging which randomizes the features used in each training subset and has decision trees as base learners. It reduces the tendency of decision trees to overfit training data by utilizing the concept of bagging and randomization. The accuracy is slightly better at 96.68%.

Finally, XGBoost is used to develop a gradient boosted model. Gradient Boosting aims to improve the model performance by training multiple machine learning models sequentially where each model learns from the mistakes of its predecessors. In general, decision trees are used as a base learner. However, it is observed that the model performance is marginally lower than the Random Forest classifier. 

Both the Gradient Boosted Classifier and the Random Forest Classifier are validated on the test set, which is totally unseen new data, where it is observed that the Random Forest Classifier (97.28%) significantly performs better than the Gradient Boosted Classifier (96.47%). It is observed that the hyper-tuning of parameters of the Random Forest classifier with grid search does not significantly increase the model performance and it is computationally expensive, hence not essential in this case. 

Finally, a detailed evaluation of the Random Forest Classifier is done on the test set. As this is a binary classification task the area under the ROC curve is obtained. The area under the ROC curve is basically the measure of the correct predictions for positive class. The ROC curve shows model performance at every classification threshold and it pltos True positive rate and false positive rate. The area under the ROC curve 0.9946 which is very close to the desired value of 1. A confusion matrix is plotted and a classification report is generated which shows various model evaluation metrics such as precision, recall and F1-score. All the evaluation metrics suggest that the model is reasonably accurate and can generate good predictions. Hence, in conclusion the Random Forest Classifier is recommended as the best model for this classification task. 
